{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf0e91a-5ed3-40cb-8770-6f3c0d3ea1d4",
   "metadata": {},
   "source": [
    "# Train and test BERT models using different parts of the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c755bf-153e-4a08-9326-9e88ec631143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from echr import *\n",
    "from nb_tfidf import *\n",
    "from bert import *\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from csv import DictWriter\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3dc8de-a4e8-4c82-ac6a-8d6b00534ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = 'results/parameter_optimization/'\n",
    "articles = ['2', '3', '5', '6', '8', '10', '11', '13', '14', 'All']\n",
    "path = 'datasets/Medvedeva/'\n",
    "json_path = 'datasets/echrod/cases.json'\n",
    "debug = False\n",
    "num_runs = 5\n",
    "cv = 10\n",
    "n_jobs = -1\n",
    "use_parts = 'facts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c667d7f3-f7a8-4d39-8706-40222d9ac9a6",
   "metadata": {
    "id": "Xy4HkhyECibW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "382ca872-c3ed-4517-a2e7-8fe6948854bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(part, article):\n",
    "    \n",
    "    def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False, debug=0):\n",
    "        \"\"\"Train the BertClassifier model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Specify loss function\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Start training loop\n",
    "        if debug: print(\"Start training...\\n\")\n",
    "        for epoch_i in range(epochs):\n",
    "            # =======================================\n",
    "            #               Training\n",
    "            # =======================================\n",
    "            # Print the header of the result table\n",
    "            if debug: print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "            if debug: print(\"-\"*70)\n",
    "\n",
    "            # Measure the elapsed time of each epoch\n",
    "            t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "            # Reset tracking variables at the beginning of each epoch\n",
    "            total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "            # Put the model into the training mode\n",
    "            model.train()\n",
    "\n",
    "            # For each batch of training data...\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                batch_counts +=1\n",
    "                # Load batch to GPU\n",
    "                b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                # Zero out any previously calculated gradients\n",
    "                model.zero_grad()\n",
    "\n",
    "                # Perform a forward pass. This will return logits.\n",
    "                logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "                # Compute loss and accumulate the loss values\n",
    "                loss = loss_fn(logits, b_labels)\n",
    "                batch_loss += loss.item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Perform a backward pass to calculate gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                # Update parameters and the learning rate\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                # Print the loss values and time elapsed for every 20 batches\n",
    "                if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                    # Calculate time elapsed for 20 batches\n",
    "                    time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                    # Print training results\n",
    "                    if debug: print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                    # Reset batch tracking variables\n",
    "                    batch_loss, batch_counts = 0, 0\n",
    "                    t0_batch = time.time()\n",
    "\n",
    "            # Calculate the average loss over the entire training data\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "            if debug: print(\"-\"*70)\n",
    "            # =======================================\n",
    "            #               Evaluation\n",
    "            # =======================================\n",
    "            if evaluation == True:\n",
    "                # After the completion of each training epoch, measure the model's performance\n",
    "                # on our validation set.\n",
    "                val_loss, val_accuracy = evaluate(model, val_dataloader, device)\n",
    "\n",
    "                # Print performance over the entire training data\n",
    "                time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "                if debug: print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "                if debug: print(\"-\"*70)\n",
    "            if debug: print(\"\\n\")\n",
    "        if debug: print(\"Training complete!\")\n",
    "\n",
    "    # Prepare the data\n",
    "    train_df = create_dataset(json_path, article, part) # echrod\n",
    "    # train_df = create_dataset(path, article, part) #medvedeva \n",
    "    train_df = balance_dataset(train_df) \n",
    "    X = train_df['text'].to_numpy()\n",
    "    y = train_df['violation'].to_numpy()\n",
    "    \n",
    "    print('Created data')\n",
    "    \n",
    "    accs, mccs, f1s = [], [], []\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    print('Running 10 folds')\n",
    "    for train_index, test_index in tqdm(skf.split(X, y)):\n",
    "        \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        #MACHINE LEARNING\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        train_inputs, train_masks = preprocessing_for_bert(X_train, tokenizer)\n",
    "        \n",
    "        set_seed(42)\n",
    "     \n",
    "        # Train\n",
    "        train_labels = torch.tensor(y_train)\n",
    "        train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "        bert_classifier, optimizer, scheduler = initialize_model(device, train_dataloader, epochs=epochs)\n",
    "        train(bert_classifier, train_dataloader, epochs=epochs)\n",
    "\n",
    "        # Test\n",
    "        test_inputs, test_masks = preprocessing_for_bert(X_test, tokenizer)\n",
    "        test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "        test_sampler = SequentialSampler(test_dataset)\n",
    "        test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)\n",
    "        probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "        # Get predictions from the probabilities\n",
    "        threshold = 0.5\n",
    "        preds = np.where(probs[:, 1] >= 0.5, 1, 0)\n",
    "        acc, mcc, f1 = return_metrics(preds, y_test, show=False)\n",
    "        accs.append(acc)\n",
    "        mccs.append(mcc)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    acc = np.mean(accs)\n",
    "    mcc = np.mean(mccs)\n",
    "    f1 = np.mean(f1s)\n",
    "    \n",
    "    field_names = ['article', 'accuracy', 'MCC', 'F1','part', 'batch_size', 'epochs', \n",
    "                   'training_size', 'train_distribution']\n",
    "    dct = {\n",
    "        'article': article,\n",
    "        'accuracy': acc,\n",
    "        'MCC': mcc,\n",
    "        'F1': f1,\n",
    "        'part': part,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs,\n",
    "        'training_size': len(train_df),\n",
    "        'train_distribution': round(train_df['violation'].mean()*100,2),\n",
    "           }\n",
    "    filename = 'results/BERT/parts/parts.csv'\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    with open(filename, 'a') as f_object:\n",
    "        dictwriter_object = DictWriter(f_object, fieldnames=field_names)\n",
    "        if not file_exists:\n",
    "            dictwriter_object.writeheader()  # file doesn't exist yet, write a header\n",
    "        dictwriter_object.writerow(dct)\n",
    "        f_object.close()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f9c3e9b-0ef6-485b-beb1-618dd1947b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procedure+facts\n",
      "\t All\n",
      "Created data\n",
      "Running 10 folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [5:24:47, 1948.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procedure\n",
      "\t All\n",
      "Created data\n",
      "Running 10 folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [4:50:49, 1744.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facts\n",
      "\t All\n",
      "Created data\n",
      "Running 10 folds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [5:20:29, 1922.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run all combinations of articles and parts\n",
    "for part in ['procedure+facts', 'procedure', 'facts']:\n",
    "    print(part)\n",
    "    for article in articles:\n",
    "        print('\\t', article)\n",
    "        run_experiment(part, article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
