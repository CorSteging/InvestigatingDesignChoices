{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d45e016-f818-4ce3-98f7-7327d0b2196d",
   "metadata": {},
   "source": [
    "# Train and test models using different parts of the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6675366-4697-4501-8da4-ad1fa4ba3d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from echr import *\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778d009b-57b6-4618-971d-e8a098b58a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {'SVM': SGDClassifier(n_jobs=-1),\n",
    "               'NB': MultinomialNB(),\n",
    "               'RF': RandomForestClassifier(n_jobs=-1),\n",
    "               'SVM_med': LinearSVC(),\n",
    "              }\n",
    "result_dir = 'results/parameter_optimization/'\n",
    "articles = ['All', '2', '3', '5', '6', '8', '10', '11', '13', '14']\n",
    "path = 'datasets/Medvedeva/'\n",
    "json_path = 'datasets/echrod/cases.json'\n",
    "debug = False\n",
    "num_runs = 5\n",
    "cv = 10\n",
    "n_jobs = -1\n",
    "use_parts = 'facts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab79b612-808f-4574-86ac-323c037d13de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF\n",
      "facts\n",
      "\t All\n",
      "\t\t--> 75.4 % 74.55 % 50.91 %\n",
      "\t 2\n",
      "\t\t--> 67.4 % 64.46 % 35.29 %\n",
      "\t 3\n",
      "\t\t--> 60.8 % 59.66 % 21.63 %\n",
      "\t 5\n",
      "\t\t--> 62.81 % 63.15 % 25.63 %\n",
      "\t 6\n",
      "\t\t--> 74.37 % 75.0 % 48.79 %\n",
      "\t 8\n",
      "\t\t--> 61.79 % 65.31 % 24.08 %\n",
      "\t 10\n",
      "\t\t--> 64.5 % 62.37 % 29.19 %\n",
      "\t 11\n",
      "\t\t--> 76.92 % 78.01 % 54.14 %\n",
      "\t 13\n",
      "\t\t--> 77.09 % 75.21 % 54.82 %\n",
      "\t 14\n",
      "\t\t--> 71.54 % 72.16 % 43.12 %\n",
      "SVM_med\n",
      "procedure+facts\n",
      "\t All\n",
      "\t\t--> 71.75 % 70.08 % 43.77 %\n",
      "\t 2\n",
      "\t\t--> 67.97 % 64.08 % 36.8 %\n",
      "\t 3\n",
      "\t\t--> 64.18 % 64.46 % 28.37 %\n",
      "\t 5\n",
      "\t\t--> 67.67 % 67.93 % 35.35 %\n",
      "\t 6\n",
      "\t\t--> 76.82 % 75.82 % 53.81 %\n",
      "\t 8\n",
      "\t\t--> 67.79 % 68.84 % 35.66 %\n",
      "\t 10\n",
      "\t\t--> 61.58 % 61.58 % 23.16 %\n",
      "\t 11\n",
      "\t\t--> 77.88 % 77.67 % 55.78 %\n",
      "\t 13\n",
      "\t\t--> 80.36 % 79.36 % 61.0 %\n",
      "\t 14\n",
      "\t\t--> 76.56 % 76.98 % 53.15 %\n",
      "procedure\n",
      "\t All\n",
      "\t\t--> 69.26 % 67.49 % 38.75 %\n",
      "\t 2\n",
      "\t\t--> 65.8 % 64.89 % 31.64 %\n",
      "\t 3\n",
      "\t\t--> 62.64 % 63.36 % 25.3 %\n",
      "\t 5\n",
      "\t\t--> 62.25 % 63.06 % 24.52 %\n",
      "\t 6\n",
      "\t\t--> 71.88 % 72.16 % 43.77 %\n",
      "\t 8\n",
      "\t\t--> 59.9 % 60.55 % 19.82 %\n",
      "\t 10\n",
      "\t\t--> 61.05 % 58.66 % 22.25 %\n",
      "\t 11\n",
      "\t\t--> 65.38 % 67.27 % 30.98 %\n",
      "\t 13\n",
      "\t\t--> 75.77 % 75.7 % 51.53 %\n",
      "\t 14\n",
      "\t\t--> 69.05 % 68.76 % 38.1 %\n",
      "facts\n",
      "\t All\n",
      "\t\t--> 72.66 % 71.18 % 45.57 %\n",
      "\t 2\n",
      "\t\t--> 66.23 % 62.68 % 33.07 %\n",
      "\t 3\n",
      "\t\t--> 64.49 % 64.37 % 29.0 %\n",
      "\t 5\n",
      "\t\t--> 65.56 % 66.14 % 31.14 %\n",
      "\t 6\n",
      "\t\t--> 77.25 % 76.38 % 54.65 %\n",
      "\t 8\n",
      "\t\t--> 68.08 % 68.32 % 36.16 %\n",
      "\t 10\n",
      "\t\t--> 62.17 % 60.82 % 24.4 %\n",
      "\t 11\n",
      "\t\t--> 74.04 % 74.29 % 48.09 %\n",
      "\t 13\n",
      "\t\t--> 77.04 % 76.32 % 54.18 %\n",
      "\t 14\n",
      "\t\t--> 73.99 % 74.28 % 48.0 %\n"
     ]
    }
   ],
   "source": [
    "for path_name, path in {'med': 'datasets/Medvedeva/', 'echrod': 'datasets/echrod/cases.json'}.items():\n",
    "    for classifier_name, classifier in classifiers.items():\n",
    "        print(classifier_name)\n",
    "        for use_parts in ['procedure+facts', 'procedure', 'facts']:\n",
    "            if classifier_name == 'RF' and use_parts != 'facts': continue\n",
    "            print(use_parts)\n",
    "            filename = 'results/parts/'+path_name+'/results_' + classifier_name + '_' + use_parts\n",
    "            results_df = []\n",
    "            for article_name in articles:\n",
    "                print('\\t', article_name)\n",
    "\n",
    "                # Build the data\n",
    "                df_complete = create_dataset(path, article_name, use_parts)\n",
    "                df_train = balance_dataset(df_complete) \n",
    "                X_train = df_train['text'].to_numpy()\n",
    "                y_train = df_train['violation'].to_numpy()\n",
    "\n",
    "\n",
    "                # Get the best params (use data params from med)\n",
    "                params = get_best_params('Article' + article_name, 'SVM_med')\n",
    "                clf_params = get_best_params('Article' + article_name, classifier_name)\n",
    "                for key, value in clf_params.items():\n",
    "                    params[key] = value\n",
    "\n",
    "                if 'norm' not in params or params['norm'] == 'None':\n",
    "                    params['norm'] = None\n",
    "                if 'use_idf' not in params:\n",
    "                    params['use_idf'] = True\n",
    "                if params['remove_stopwords']:\n",
    "                    params['remove_stopwords'] = 'english'\n",
    "                else:\n",
    "                    params['remove_stopwords'] = None\n",
    "                if not type(params['ngram_range']) == int:\n",
    "                    params['ngram_range'] = ast.literal_eval(params['ngram_range'])\n",
    "                params['parts'] = '+'.join(ast.literal_eval(params['parts']))\n",
    "\n",
    "\n",
    "                vec = ('wordvec', TfidfVectorizer(analyzer = 'word', \n",
    "                                                  ngram_range = params['ngram_range'], \n",
    "                                                  binary = params['binary'], \n",
    "                                                  lowercase = params['remove_upper'],\n",
    "                                                  min_df = params['min_df'], \n",
    "                                                  norm = params['norm'], \n",
    "                                                  stop_words = params['remove_stopwords'], \n",
    "                                                  use_idf = params['use_idf']))\n",
    "                clf = get_classifier('Article'+article_name, classifier_name)\n",
    "\n",
    "                accs, mccs, f1s = [], [], []\n",
    "                for idx in range(0, num_runs):\n",
    "                    acc, mcc, f1 = train_model_cross_val(X_train, y_train, vec, clf, debug=debug, cv=cv, n_jobs=n_jobs)\n",
    "                    accs.append(acc)\n",
    "                    mccs.append(mcc)\n",
    "                    f1s.append(f1)\n",
    "\n",
    "                results_df.append({\n",
    "                    'Article' : article_name,\n",
    "                    'Classifier' : classifier_name,\n",
    "                    'Accuracy' : np.mean(accs),\n",
    "                    'MCC' : np.mean(mccs),\n",
    "                    'F1' : np.mean(f1s),\n",
    "                    'part': use_parts,\n",
    "                    'training_size': len(df_train),\n",
    "                    'train_distribution': round(df_train['violation'].mean()*100,2),\n",
    "                })\n",
    "                print('\\t\\t-->', round(np.mean(accs)*100, 2), '%', round(np.mean(f1s)*100, 2), '%', round(np.mean(mccs)*100, 2), '%')\n",
    "            pd.DataFrame(results_df).to_csv(filename + '.csv', index=False)\n",
    "#             with open(filename+'.pkl', 'wb') as fid:\n",
    "#                 pickle.dump(classifier, fid)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
